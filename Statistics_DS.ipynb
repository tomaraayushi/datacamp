{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmqw7x5ARshN3EZyeku63c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomaraayushi/datacamp/blob/main/Statistics_DS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFpRsIh_n8j_"
      },
      "outputs": [],
      "source": [
        "#Mean & Median\n",
        "\n",
        "# Import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# Subset for Belgium and USA only\n",
        "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") |\n",
        "                                (food_consumption['country'] == \"USA\")]\n",
        "\n",
        "# Group by country, select consumption column, and compute mean and median\n",
        "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Variance and SD\n",
        "\n",
        "# Print variance and sd of co2_emission for each food_category\n",
        "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'beef'\n",
        "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'eggs'\n",
        "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "KD-64pcqSEgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quantiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper cutoffs for outliers\n",
        "lower = q1 - 1.5 * iqr\n",
        "upper = q3 + 1.5 * iqr\n",
        "\n",
        "# Subset emissions_by_country to find outliers\n",
        "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "iJWc0R4EU_1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create probability distribution\n",
        "size_dist =  restaurant_groups.groupby('group_size')['group_size'].count()/restaurant_groups['group_size'].count()\n",
        "OR\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index(drop=True)\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "print(size_dist)\n",
        "\n",
        "# Subset groups of size 4 or more\n",
        "groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
        "78\n",
        "# Sum the probabilities of groups_4_or_more\n",
        "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
        "print(prob_4_or_more)"
      ],
      "metadata": {
        "id": "kU0TYne5Ynyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "# Import uniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Generate 1000 wait times between 0 and 30 mins\n",
        "wait_times = uniform.rvs(0, 30, size=1000)\n",
        "\n",
        "# Create a histogram of simulated times and show plot\n",
        "plt.hist(wait_times)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bqEYxkOBJf8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Binomial Distribution\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "# Simulate a single deal\n",
        "print(binom.rvs(1, 0.3, size=1))\n",
        "\n",
        "# Simulate 1 week of 3 deals\n",
        "print(binom.rvs(3, 0.3, size=1))\n",
        "\n",
        "# Simulate 52 weeks of 3 deals\n",
        "deals = binom.rvs(3, 0.3, size=52)\n",
        "\n",
        "# Print mean deals won per week\n",
        "print(np.mean(deals))\n",
        "\n",
        "# Probability of closing 3 out of 3 deals\n",
        "prob_3 = binom.pmf(3, 3, 0.3)\n",
        "\n",
        "print(prob_3)\n",
        "\n",
        "# Probability of closing > 1 deal out of 3 deals\n",
        "prob_greater_than_1 = 1- binom.cdf(1, 3, 0.3)\n",
        "\n",
        "print(prob_greater_than_1)\n",
        "\n",
        "# Expected number won with 30% win rate\n",
        "won_30pct =  3*0.3\n",
        "print(won_30pct)\n",
        "\n",
        "# Expected number won with 25% win rate\n",
        "won_25pct = 3*0.25\n",
        "print(won_25pct)\n",
        "\n",
        "# Expected number won with 35% win rate\n",
        "won_35pct = 3*0.35\n",
        "print(won_35pct)"
      ],
      "metadata": {
        "id": "Hvg95K7RRFYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normal Distribution\n",
        "# Calculate new average amount\n",
        "new_mean = 5000 + (5000*0.2)\n",
        "\n",
        "# Calculate new standard deviation\n",
        "new_sd = 2000 + (2000*0.3)\n",
        "\n",
        "# Simulate 36 new sales\n",
        "new_sales = norm.rvs(new_mean, new_sd, size=36)\n",
        "\n",
        "# Create histogram and show\n",
        "plt.hist(new_sales)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4ix5YpbacFPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Central Limit Theorem\n",
        "# Set seed to 104\n",
        "np.random.seed(104)\n",
        "\n",
        "# Sample 20 num_users with replacement from amir_deals and take mean\n",
        "samp_20 = amir_deals['num_users'].sample(20, replace=True)\n",
        "np.mean(samp_20)\n",
        "\n",
        "sample_means = []\n",
        "# Loop 100 times\n",
        "for i in range(100):\n",
        "  # Take sample of 20 num_users\n",
        "  samp_20 = amir_deals['num_users'].sample(20, replace= True)\n",
        "  # Calculate mean of samp_20\n",
        "  samp_20_mean = np.mean(samp_20)\n",
        "  # Append samp_20_mean to sample_means\n",
        "  sample_means.append(samp_20_mean)\n",
        "\n",
        "print(sample_means)\n",
        "\n",
        "# Convert to Series and plot histogram\n",
        "sample_means_series = pd.Series(sample_means)\n",
        "sample_means_series.hist()\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "#Mean of Means\n",
        "# Set seed to 321\n",
        "np.random.seed(321)\n",
        "\n",
        "sample_means = []\n",
        "# Loop 30 times to take 30 means\n",
        "for i in range(30):\n",
        "  # Take sample of size 20 from num_users col of all_deals with replacement\n",
        "  cur_sample = all_deals['num_users'].sample(20, replace=True)\n",
        "  # Take mean of cur_sample\n",
        "  cur_mean = np.mean(cur_sample)\n",
        "  # Append cur_mean to sample_means\n",
        "  sample_means.append(cur_mean)\n",
        "\n",
        "# Print mean of sample_means\n",
        "print(np.mean(sample_means))\n",
        "\n",
        "# Print mean of num_users in amir_deals\n",
        "print(np.mean(amir_deals['num_users']))"
      ],
      "metadata": {
        "id": "WluY3i7vvqjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation\n",
        "# Create scatterplot of happiness_score vs life_exp with trendline\n",
        "sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Correlation between life_exp and happiness_score\n",
        "cor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n",
        "\n",
        "print(cor)"
      ],
      "metadata": {
        "id": "0v1GGKVXwwWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}